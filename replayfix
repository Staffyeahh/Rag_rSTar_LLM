import os
import sys
import random
import math
from collections import deque
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import gym
from gym import spaces
from sklearn.preprocessing import MinMaxScaler
from tqdm import tqdm  # Ændret fra tqdm.notebook til tqdm

import torch
import torch.nn as nn
import torch.optim as optim

import logging
from datetime import datetime

# 3. For GPU overvågning (valgfrit)
try:
    import pynvml
    pynvml.nvmlInit()
    n_gpus = pynvml.nvmlDeviceGetCount()
except ImportError:
    n_gpus = 0
except pynvml.NVMLError:
    n_gpus = 0

# 4. Konfigurer logging til kun at skrive til filer
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_dir = r'/mnt/e/crypto/RF_model/agent/bootcamp'  # Din output mappe
log_filename = os.path.join(output_dir, f'dqn_her_{timestamp}.log')

# Fjern eventuelle eksisterende loggere
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

logging.basicConfig(
    filename=log_filename,
    filemode='a',
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger()

# 5. Import nødvendige biblioteker til tekniske indikatorer
from ta import trend, momentum, volatility
from ta.volatility import AverageTrueRange
from ta.trend import MACD, IchimokuIndicator

# ----------------------------
# SumTree Datastruktur
# ----------------------------
class SumTree:
    """
    SumTree data structure for efficient prioritized sampling.
    """
    def __init__(self, capacity):
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1)
        self.data = np.zeros(capacity, dtype=object)
        self.write = 0

    def add(self, priority, data):
        idx = self.write + self.capacity - 1
        self.data[self.write] = data
        self.update(idx, priority)
        self.write += 1
        if self.write >= self.capacity:
            self.write = 0
        return idx  # Return the index of the added data

    def update(self, idx, priority):
        change = priority - self.tree[idx]
        self.tree[idx] = priority
        while idx != 0:
            idx = (idx - 1) // 2
            self.tree[idx] += change

    def get(self, s):
        idx = 0
        while idx < self.capacity - 1:
            left = 2 * idx + 1
            right = left + 1
            if s <= self.tree[left]:
                idx = left
            else:
                s -= self.tree[left]
                idx = right
        data_idx = idx - self.capacity + 1
        return (idx, self.tree[idx], self.data[data_idx])

    @property
    def total_priority(self):
        return self.tree[0]

# ----------------------------
# Prioritized Replay Buffer med HER
# ----------------------------
class PrioritizedReplayBufferHER:
    """
    Prioritized Replay Buffer med Hindsight Experience Replay (HER).
    """
    def __init__(self, capacity, alpha=0.6, her_k=4):
        self.capacity = capacity
        self.alpha = alpha
        self.her_k = her_k  # Antal hindsight mål pr. transition
        self.sum_tree = SumTree(capacity)
        self.max_priority = 1.0
        self.achieved_goals = deque(maxlen=1000)  # Buffer til opnåede mål

    def add(self, state, action, reward, next_state, done, goal, net_worth):
        """Tilføjer en transition til bufferen."""
        data = (state, action, reward, next_state, done, goal, net_worth)
        priority = self.max_priority ** self.alpha
        idx = self.sum_tree.add(priority, data)
        if done:
            self.achieved_goals.append(net_worth)
        return idx  # Return the index of the added data

    def sample(self, batch_size, beta=0.4):
        """Sampler en batch med transitions, inklusive HER."""
        batch = []
        indices = []
        priorities = []
        segment = self.sum_tree.total_priority / batch_size
        for i in range(batch_size):
            a = segment * i
            b = segment * (i + 1)
            s = random.uniform(a, b)
            idx, priority, data = self.sum_tree.get(s)
            batch.append(data)
            indices.append(idx)
            priorities.append(priority)

        # Beregn sampling probabiliteter og IS-vægte
        sampling_probabilities = np.array(priorities) / self.sum_tree.total_priority
        is_weight = np.power(self.capacity * sampling_probabilities, -beta)
        is_weight /= is_weight.max()

        # Implementer HER
        her_transitions = []
        for transition in batch:
            state, action, reward, next_state, done, goal, net_worth = transition
            if done and self.her_k > 0 and len(self.achieved_goals) > 0:
                for _ in range(self.her_k):
                    her_goal = random.choice(self.achieved_goals)
                    her_reward = self.calculate_her_reward(her_goal, net_worth)
                    her_transition = (state, action, her_reward, next_state, done, her_goal, net_worth)
                    her_priority = self.max_priority ** self.alpha
                    her_idx = self.sum_tree.add(her_priority, her_transition)
                    batch.append(her_transition)
                    indices.append(her_idx)
                    priorities.append(her_priority)
                    sampling_probabilities = np.append(sampling_probabilities, her_priority / self.sum_tree.total_priority)
                    is_weight = np.append(is_weight, (self.capacity * (her_priority / self.sum_tree.total_priority)) ** -beta)
        
        # Juster is_weight hvis nødvendigt
        is_weight /= is_weight.max()

        # Juster batch hvis nødvendigt
        if len(batch) > batch_size:
            batch = batch[:batch_size]
            indices = indices[:batch_size]
            is_weight = is_weight[:batch_size]

        states, actions, rewards, next_states, dones, goals, net_worths = zip(*batch)
        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones),
            np.array(goals),
            indices,
            is_weight
        )


    def calculate_her_reward(self, her_goal, net_worth):
        """
        Beregner HER belønning baseret på det nye mål.
        Dette skal tilpasses dit specifikke miljø.
        """
        # Eksempel: hvis net worth >= her_goal, giv en ekstra belønning
        if net_worth >= her_goal:
            return 100  # Ekstra belønning for at opnå eller overstige målet
        else:
            return -10  # Straf hvis målet ikke er opnået

    def update_priorities(self, indices, priorities):
        """Opdaterer prioriteringerne for transitions i batchen."""
        for idx, priority in zip(indices, priorities):
            priority = (priority + 1e-5) ** self.alpha
            self.sum_tree.update(idx, priority)
            self.max_priority = max(self.max_priority, priority)



# ----------------------------
# Custom Sampler Klasse
# ----------------------------
from torch.utils.data import Sampler



# ----------------------------
# Custom Gym Miljø (`SimplifiedCryptoTradingEnv`)
# ----------------------------
class SimplifiedCryptoTradingEnv(gym.Env):
    """
    Simplificeret handelsmiljø til kryptovaluta ved hjælp af OpenAI Gym med HER.
    """

    metadata = {'render.modes': ['human']}

    def __init__(self, df, initial_balance=10000, lookback_window_size=50, transaction_cost=0.001, scaler=None):
        super(SimplifiedCryptoTradingEnv, self).__init__()

        self.df = df.reset_index(drop=True)
        self.initial_balance = initial_balance
        self.lookback_window_size = lookback_window_size
        self.transaction_cost = transaction_cost
        self.scaler = scaler

        # Action space: 0 = Hold, 1 = Buy, 2 = Sell
        self.action_space = spaces.Discrete(3)

        # Observation space: markedsdata + net_worth + goal
        # Antag at 'df' indeholder alle nødvendige features
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.lookback_window_size, self.df.shape[1] + 2),  # +2 for net_worth og goal
            dtype=np.float32
        )

        self.reset()

    def reset(self):
        """Reset miljøet for en ny episode."""
        self.balance = self.initial_balance
        self.net_worth = self.initial_balance
        self.prev_net_worth = self.initial_balance  # For belønningsberegning
        self.position = 0.0  # Antal enheder holdt
        self.entry_price = 0.0
        self.current_step = self.lookback_window_size
        self.trade_history = []
        self.goals = []
        self.achieved_goals = []

        # Sæt et nyt mål for denne episode
        self.goal = self.net_worth + random.uniform(-500, 500)  # Mål kan være +-500 fra initial balance
        self.goals.append(self.goal)

        logger.info(f"Miljø reset: Net Worth={self.net_worth}, Balance={self.balance}, Goal={self.goal}")

        return self._next_observation()

    def _next_observation(self):
        """Genererer den næste observation inklusive net_worth og mål."""
        obs = self.df.iloc[self.current_step - self.lookback_window_size:self.current_step].values
        net_worth_array = np.full((self.lookback_window_size, 1), self.net_worth)
        goal_array = np.full((self.lookback_window_size, 1), self.goal)
        return np.hstack((obs, net_worth_array, goal_array))

    def _get_current_price(self):
        """Henter den aktuelle pris."""
        try:
            return float(self.df.loc[self.current_step, 'close_unscaled'])
        except KeyError:
            raise ValueError(f"No price data available for step {self.current_step}")
        except ValueError:
            raise ValueError("Invalid price data encountered")

    def step(self, action):
        """
        Udfører en handling og opdaterer miljøet.

        Args:
            action (int): 0 = Hold, 1 = Buy, 2 = Sell

        Returns:
            tuple: next_observation, reward, done, info
        """
        done = False
        reward = 0

        # Tjek om current_step er inden for datasættets grænser
        if self.current_step >= len(self.df):
            done = True
            if self.position > 0:
                # Hvis der stadig er en åben position, sælg den ved slutningen
                last_price = self.df.iloc[-1]['close_unscaled']
                exit_price = last_price * (1 - self.transaction_cost)
                profit = (exit_price - self.entry_price) * self.position
                self.balance += self.position * exit_price
                self.net_worth = self.balance
                self.trade_history.append({
                    'type': 'sell',
                    'step': self.current_step,
                    'price': last_price,
                    'profit': profit,
                    'units': self.position
                })
                reward += self.shape_reward(profit)  # Belønning med shaping
                self.achieved_goals.append(self.net_worth)
                logger.debug(f"Sell på pris {last_price} ved slutning, Profit: {profit}")
            return self._next_observation(), reward, done, {
                'net_worth': self.net_worth,
                'balance': self.balance,
                'position': self.position,
                'goal': self.goal
            }

        # Hent den aktuelle pris
        try:
            current_price = self._get_current_price()
        except ValueError as e:
            logger.error(e)
            done = True
            return self._next_observation(), reward, done, {
                'net_worth': self.net_worth,
                'balance': self.balance,
                'position': self.position,
                'goal': self.goal
            }

        # Udfør handlingen
        if action == 1:
            # Buy
            max_cost = 0.02 * self.balance  # Maksimal kostnad pr. handel: 2% af balance
            cost_per_unit = current_price * (1 + self.transaction_cost)
            units = max_cost / cost_per_unit
            if units > 0:
                self.balance -= units * cost_per_unit
                self.position += units
                self.entry_price = current_price * (1 + self.transaction_cost)
                self.trade_history.append({
                    'type': 'buy',
                    'step': self.current_step,
                    'price': current_price,
                    'units': units
                })
                reward = 0  # Ingen umiddelbar belønning for at købe
                logger.debug(f"Buy {units:.4f} units på pris {current_price}")
            else:
                # Ikke muligt at købe
                reward = -1  # Straf for at forsøge at købe uden nok balance
                logger.debug(f"Forsøg på at købe uden nok balance: Balance={self.balance}, Max Cost={max_cost}, Current Price={current_price}")
        elif action == 2 and self.position > 0:
            # Sell
            exit_price = current_price * (1 - self.transaction_cost)
            profit = (exit_price - self.entry_price) * self.position
            self.balance += self.position * exit_price
            self.net_worth = self.balance
            self.trade_history.append({
                'type': 'sell',
                'step': self.current_step,
                'price': current_price,
                'profit': profit,
                'units': self.position
            })
            reward = self.shape_reward(profit)  # Belønning er nu direkte proportional med profit
            self.achieved_goals.append(self.net_worth)
            logger.debug(f"Sell på pris {current_price}, Profit: {profit}, Units: {self.position}")
            self.position = 0.0
        else:
            # Hold eller ugyldig handling
            reward = 0  # Ingen belønning eller straf for at holde
            if self.position > 0:
                unrealized_profit = (current_price * (1 - self.transaction_cost) - self.entry_price) * self.position
                self.net_worth = self.balance + self.position * (current_price * (1 - self.transaction_cost))
                # Du kan tilføje en lille straf for at holde positionen for længe, hvis ønsket
                # reward -= 0.01  # Eksempel på lille straf for inaktivitet

        # Inkrementer step
        self.current_step += 1

        # Check for slutning af data
        if self.current_step >= len(self.df):
            done = True
            if self.position > 0:
                # Hvis der stadig er en åben position, sælg den ved slutningen
                last_price = self.df.iloc[-1]['close_unscaled']
                exit_price = last_price * (1 - self.transaction_cost)
                profit = (exit_price - self.entry_price) * self.position
                self.balance += self.position * exit_price
                self.net_worth = self.balance
                self.trade_history.append({
                    'type': 'sell',
                    'step': self.current_step,
                    'price': last_price,
                    'profit': profit,
                    'units': self.position
                })
                reward += self.shape_reward(profit)  # Belønning med shaping
                self.achieved_goals.append(self.net_worth)
                logger.debug(f"Sell på pris {last_price} ved slutning, Profit: {profit}")

        # Log kun relevante metrics til konsollen
        # Net_Worth og Total_Reward vil blive logget i hovedtræningsloopet

        return self._next_observation(), reward, done, {
            'net_worth': self.net_worth,
            'balance': self.balance,
            'position': self.position,
            'goal': self.goal
        }

    def shape_reward(self, profit):
        """
        Reward Shaping: Giver belønning proportional med profit.

        Args:
            profit (float): Profit fra en handel.

        Returns:
            float: Shaped reward.
        """
        return profit  # Direkte proportional belønning

    def render(self, mode='human'):
        """Render miljøet."""
        if mode == 'human':
            profit = self.net_worth - self.initial_balance
            print(f'\nStep: {self.current_step}')
            print(f'Balance: ${self.balance:.2f}')
            print(f'Net Worth: ${self.net_worth:.2f}')
            print(f'Profit: ${profit:.2f}')
            print(f'Position: {self.position:.4f} units')
            print(f'Goal: ${self.goal:.2f}')

# ----------------------------
# Dueling DQN Model med Flere Lag og Neuroner
# ----------------------------
class DuelingDQNModelHER(nn.Module):
    """
    Dueling Deep Q-Network Model til HER med øgede lag og neuroner.
    """

    def __init__(self, state_shape, action_size, device):
        super(DuelingDQNModelHER, self).__init__()
        self.device = device

        # Korrekt input size beregning
        input_size = state_shape[0] * state_shape[1]  # Fjernet +2 for net_worth og goal

        self.feature_layers = nn.Sequential(
            nn.Linear(input_size, 512),
            nn.ReLU(),
            nn.LayerNorm(512),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Dropout(0.2)
        )

        # Value stream
        self.value_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Dropout(0.2),
            nn.Linear(128, 1)
        )

        # Advantage stream
        self.advantage_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Dropout(0.2),
            nn.Linear(128, action_size)
        )

    def forward(self, x):
        """
        Forward pass gennem modellen.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Q-værdier for hver handling.
        """
        x = x.to(self.device)
        x = x.reshape(x.size(0), -1)
        features = self.feature_layers(x)
        value = self.value_stream(features)
        advantage = self.advantage_stream(features)
        # Kombiner streams for at få endelige Q-værdier
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        return q_values

# ----------------------------
# DQN Agent med HER


class DQNAgentHER:
    """
    DQN Agent med Prioritized Experience Replay og Hindsight Experience Replay (HER).
    """
    def __init__(self, state_shape, action_size,
                 lr, gamma,
                 epsilon, epsilon_min, epsilon_decay,
                 batch_size, memory_size,
                 her_k=4,
                 model_path=None, device=torch.device('cpu')):
        """
        Initialiserer DQN-agenten med HER.

        Args:
            state_shape (tuple): Formen på tilstandsrummet.
            action_size (int): Antallet af mulige handlinger.
            lr (float): Læringsraten for optimizeren.
            gamma (float): Diskonteringsfaktor for belønninger.
            epsilon (float): Startværdi for epsilon i epsilon-greedy politikken.
            epsilon_min (float): Minimum værdi for epsilon.
            epsilon_decay (float): Faktor for nedgang af epsilon efter hver iteration.
            batch_size (int): Størrelsen på minibatchen til replay.
            memory_size (int): Maksimal størrelse af replay buffer.
            her_k (int): Antal HER mål pr. transition.
            model_path (str, optional): Sti til en gemt model at indlæse.
            device (torch.device, optional): Enheden (CPU/GPU) til at køre modellen på.
        """
        self.state_shape = state_shape
        self.action_size = action_size
        self.memory = PrioritizedReplayBufferHER(memory_size, alpha=0.6, her_k=her_k)
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay  # Dette vil nu blive håndteret i hovedtræningen
        self.batch_size = batch_size
        self.train_start = 1000  # Øget træningsstart for bedre buffer
        self.target_update_counter = 0
        self.target_update_frequency = 516  # Øget target update frequency
        self.buffer_full_logged = False
        self.device = device
        self.beta = 0.7608423670750375
        self.beta_increment_per_sampling = 0.00688155337657235  # Langsommere inkrement

        try:
            self.model = DuelingDQNModelHER(state_shape, action_size, device).to(device)
            self.target_model = DuelingDQNModelHER(state_shape, action_size, device).to(device)
            self.update_target_model()
        except Exception as e:
            logger.error(f"Fejl ved initialisering af modeller: {e}")
            raise

        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)
        self.criterion = nn.SmoothL1Loss(reduction='none')  # Tilpas reducering til prioriteret replay
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=1000, verbose=True)

        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
            logger.info(f"Model indlæst fra {model_path}")

        

    def update_target_model(self):
        """Opdaterer target netværket med vægtene fra hovednetværket."""
        self.target_model.load_state_dict(self.model.state_dict())
        logger.info("Target model opdateret")

    def remember(self, state, action, reward, next_state, done, goal, net_worth):
        """
        Gemmer erfaringer i replay buffer.

        Args:
            state (np.array): Aktuel tilstand.
            action (int): Udført handling.
            reward (float): Belønning modtaget.
            next_state (np.array): Næste tilstand.
            done (bool): Om episoden er afsluttet.
            goal (float): Aktuelt mål.
            net_worth (float): Aktuel nettoformue.
        """
        self.memory.add(state, action, reward, next_state, done, goal, net_worth)
        if not self.buffer_full_logged and len(self.memory.sum_tree.data) >= self.train_start and len(self.memory.sum_tree.data) % 1000 == 0:
            logger.info(f"Replay buffer størrelse: {len(self.memory.sum_tree.data)}")
            if len(self.memory.sum_tree.data) == self.memory.capacity:
                self.buffer_full_logged = True

    def act(self, state):
        """
        Vælger en handling baseret på epsilon-greedy strategi.

        Args:
            state (np.array): Aktuel tilstand.

        Returns:
            int: Valgt handling.
        """
        if np.random.rand() <= self.epsilon:
            action = np.random.choice(self.action_size)
            return action
        else:
            self.model.eval()
            with torch.no_grad():
                state = torch.FloatTensor(state).unsqueeze(0).to(self.device, non_blocking=True)
                q_values = self.model(state)
            self.model.train()
            return torch.argmax(q_values[0]).item()
    def replay(self):
        """Udfører experience replay for at træne netværket."""
        if len(self.memory.sum_tree.data) < self.train_start:
            logger.debug(f"Replay buffer ikke fyldt endnu: {len(self.memory.sum_tree.data)}/{self.train_start}")
            return
        
        
        try:
            # Samplér en batch fra replay buffer med HER
            states, actions, rewards, next_states, dones, goals, indices, is_weights = self.memory.sample(self.batch_size, beta=self.beta)

            # Konverter til torch tensors og flyt til GPU
            states = torch.FloatTensor(states).to(self.device)
            actions = torch.LongTensor(actions).to(self.device)
            rewards = torch.FloatTensor(rewards).to(self.device)
            next_states = torch.FloatTensor(next_states).to(self.device)
            dones = torch.FloatTensor(dones.astype(int)).to(self.device)
            goals = torch.FloatTensor(goals).unsqueeze(1).to(self.device)
            is_weights = torch.FloatTensor(is_weights).to(self.device)

            # Beregn Q-values og target Q-values
            current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)

            with torch.no_grad():
                next_actions = self.model(next_states).max(1)[1].unsqueeze(1)
                next_q_values = self.target_model(next_states).gather(1, next_actions).squeeze(1)

            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values

            # Beregn loss med vægtning
            loss = self.criterion(current_q_values, target_q_values.detach())
            loss = loss * is_weights
            prios = loss + 1e-5  # Prioritetsberegning
            loss = loss.mean()

            self.optimizer.zero_grad()
            loss.backward()

            # Clip gradients
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
            self.optimizer.step()
            self.scheduler.step(loss)  # Scheduler baseret på loss

            # Opdater prioriteringerne i replay buffer
            self.memory.update_priorities(indices, prios.data.cpu().numpy())

            logger.debug(f"Loss: {loss.item()} ved step {self.target_update_counter}")

            self.target_update_counter += 1
            if self.target_update_counter % self.target_update_frequency == 0:
                self.update_target_model()

        except Exception as e:
            logger.error(f"Fejl i replay: {e}")
            raise

    
    def save_model(self, filepath):
        """
        Gemmer modelens vægte til en fil.

        Args:
            filepath (str): Sti til filen hvor modellen skal gemmes.
        """
        torch.save(self.model.state_dict(), filepath)
        logger.info(f"Model gemt til {filepath}")

    def load_model(self, filepath):
        """
        Indlæser modelens vægte fra en fil.

        Args:
            filepath (str): Sti til filen hvor modellen skal indlæses fra.
        """
        self.model.load_state_dict(torch.load(filepath, map_location=self.device))
        self.target_model.load_state_dict(torch.load(filepath, map_location=self.device))
        logger.info(f"Model indlæst fra {filepath}")

# ----------------------------
# Helper Funktioner for Logging og Plotting
# ----------------------------
def plot_trades(trade_history, df, output_dir, csv_path='trade_history.csv'):
    """
    Plotter handleresultater og gemmer handelsaktivitet til CSV.

    Args:
        trade_history (list): Liste over handler.
        df (pd.DataFrame): DataFrame med markedsdata.
        output_dir (str): Sti til output mappen.
        csv_path (str, optional): Filnavn til CSV-filen.
    """
    buys = [trade for trade in trade_history if trade['type'] == 'buy']
    sells = [trade for trade in trade_history if trade['type'] == 'sell']

    # Gem trade history til CSV
    df_trades = pd.DataFrame(trade_history)
    trade_csv_path = os.path.join(output_dir, csv_path)
    df_trades.to_csv(trade_csv_path, index=False)
    logger.info(f"Handelsaktiviteter gemt til {trade_csv_path}")

    plt.figure(figsize=(12,6))
    # Brug unscaled 'close' priser
    plt.plot(df['close_unscaled'].values, label='Close Price')

    if buys:
        buy_steps = [trade['step'] for trade in buys]
        buy_prices = [trade['price'] for trade in buys]
        plt.scatter(buy_steps, buy_prices, marker='^', color='g', label='Buy', alpha=1)

    if sells:
        sell_steps = [trade['step'] for trade in sells]
        sell_prices = [trade['price'] for trade in sells]
        plt.scatter(sell_steps, sell_prices, marker='v', color='r', label='Sell', alpha=1)

    plt.title('Trading Actions')
    plt.xlabel('Step')
    plt.ylabel('Price')
    plt.legend()
    trading_plot_path = os.path.join(output_dir, 'trading_actions.png')
    plt.savefig(trading_plot_path)
    plt.show()
    logger.info(f"Handelsaktivitets plot gemt til {trading_plot_path}")

def plot_training_results(episode_net_worths, episode_rewards, output_dir, csv_path='training_results.csv'):
    """
    Plotter net worth og total reward over træningsepisoder og gemmer resultater til CSV.

    Args:
        episode_net_worths (list): Liste over net worth pr. episode.
        episode_rewards (list): Liste over total belønning pr. episode.
        output_dir (str): Sti til output mappen.
        csv_path (str, optional): Filnavn til CSV-filen.
    """
    df_results = pd.DataFrame({
        'Episode': range(1, len(episode_net_worths) + 1),
        'Net_Worth': episode_net_worths,
        'Total_Reward': episode_rewards
    })
    training_csv_path = os.path.join(output_dir, csv_path)
    df_results.to_csv(training_csv_path, index=False)
    logger.info(f"Træningsresultater gemt til {training_csv_path}")

    # Plot Net Worth Over Episodes
    plt.figure(figsize=(12,6))
    plt.plot(df_results['Episode'], df_results['Net_Worth'], label='Net Worth')
    plt.title('Net Worth Over Episodes')
    plt.xlabel('Episode')
    plt.ylabel('Net Worth')
    plt.legend()
    net_worth_plot_path = os.path.join(output_dir, 'net_worth_over_episodes.png')
    plt.savefig(net_worth_plot_path)
    plt.show()
    logger.info(f"Net Worth plot gemt til {net_worth_plot_path}")

    # Plot Total Reward Over Episodes
    plt.figure(figsize=(12,6))
    plt.plot(df_results['Episode'], df_results['Total_Reward'], label='Total Reward', color='orange')
    plt.title('Total Reward Over Episodes')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.legend()
    total_reward_plot_path = os.path.join(output_dir, 'total_reward_over_episodes.png')
    plt.savefig(total_reward_plot_path)
    plt.show()
    logger.info(f"Total Reward plot gemt til {total_reward_plot_path}")

def log_gpu_usage(gpu_id):
    """
    Logger GPU hukommelsesbrug.

    Args:
        gpu_id (int): GPU ID.
    """
    try:
        handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_id)
        info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        logger.info(f"GPU {gpu_id} Memory Usage: {info.used / 1024 ** 2} MB / {info.total / 1024 ** 2} MB")
    except pynvml.NVMLError as e:
        logger.warning(f"Fejl ved hentning af GPU info: {e}")

# ----------------------------
# Evaluer Modellens Præstation på Valideringssæt
# ----------------------------
def evaluate_model(df_val, agent, output_dir):
    """
    Evaluerer agentens præstation på valideringssæt.

    Args:
        df_val (pd.DataFrame): DataFrame med valideringsdata.
        agent (DQNAgentHER): Trænet DQN-agent.
        output_dir (str): Sti til output mappen.
    """
    env_val = SimplifiedCryptoTradingEnv(df_val, transaction_cost=agent.memory.sum_tree.capacity, scaler=None)
    total_profit = 0
    episodes = 10

    for e in range(episodes):
        state = env_val.reset()
        done = False
        while not done:
            action = agent.act(state)
            next_state, reward, done, info = env_val.step(action)
            total_profit += reward
            state = next_state

    average_profit = total_profit / episodes
    logger.info(f"Validerings Profit over {episodes} episoder: {average_profit}")
    # Udskriv kun, hvis du ønsker det i konsollen
    print(f"Validerings Profit over {episodes} episoder: {average_profit}")

# ----------------------------
# Hovedtræningsfunktion
# ----------------------------
def main_training(df_train, df_val, scaler, output_dir):
    """
    Udfører hovedtræningen af DQN-agenten med HER og faste hyperparametre.

    Args:
        df_train (pd.DataFrame): DataFrame med træningsdata.
        df_val (pd.DataFrame): DataFrame med valideringsdata.
        scaler (MinMaxScaler): Skaleringsobjekt til data.
        output_dir (str): Sti til output mappen.
    """
    # Faste hyperparametre
    best_params = {
        'lr': 2.4333262941686903e-05, 
        'gamma': 0.9040589942764626,
        'epsilon': 1.0,             # Initial epsilon
        'epsilon_min': 0.05,        # Minimum epsilon
        'epsilon_decay': 0.99999064,     # Denne vil nu blive håndteret anderledes
        'batch_size': 128,
        'train_interval': 5,
        'her_k': 4                  # Antal HER mål pr. transition
    }

    # Bestem hvilken GPU der skal bruges
    if n_gpus >= 1:
        training_device = torch.device('cuda:0')
        gpu_id = 0
        logger.info("Bruger GPU 0 til træning.")
    else:
        training_device = torch.device('cpu')
        gpu_id = None
        logger.info("GPU ikke tilgængelig. Bruger CPU til træning.")

    # Initialiser miljøet
    env_full = SimplifiedCryptoTradingEnv(
        df=df_train,
        initial_balance=10000,
        lookback_window_size=50,
        transaction_cost=0.001,
        scaler=scaler
    )

    # Initialiser DQN Agent med HER
    agent = DQNAgentHER(
        state_shape=env_full.observation_space.shape,
        action_size=env_full.action_space.n,
        lr=best_params['lr'],
        gamma=best_params['gamma'],
        epsilon=best_params['epsilon'],
        epsilon_min=best_params['epsilon_min'],
        epsilon_decay=best_params['epsilon_decay'],  # Denne bliver ikke brugt direkte
        batch_size=best_params['batch_size'],
        memory_size=200000,
        her_k=best_params['her_k'],
        device=training_device
    )

    episodes = 100  # Øget antallet af episoder for mere omfattende træning
    best_net_worth = env_full.initial_balance
    episode_rewards = []
    episode_net_worths = []

    patience = 50  # Øg patience for early stopping
    min_delta = 100  # Øg min_delta til at kræve større forbedring
    wait = 0
    best_val_reward = -math.inf  # Initialize til minus uendelig

    # Beregn decay_rate for eksponentiel decay baseret på total episodes
    epsilon_start = best_params['epsilon']
    epsilon_min = best_params['epsilon_min']
    total_episodes = episodes
    decay_rate = -math.log(epsilon_min / epsilon_start) / total_episodes

    for e in tqdm(range(episodes), desc='Episodes'):
        state = env_full.reset()
        done = False
        total_reward = 0
        step = 0

        while not done:
            action = agent.act(state)
            next_state, reward, done, info = env_full.step(action)
            agent.remember(state, action, reward, next_state, done, info['goal'], info['net_worth'])
            state = next_state
            total_reward += reward

            step += 1

        episode_rewards.append(total_reward)
        episode_net_worths.append(env_full.net_worth)

        # Opdater epsilon efter hver episode
        agent.epsilon = epsilon_min + (epsilon_start - epsilon_min) * math.exp(-decay_rate * (e + 1))
        agent.epsilon = max(agent.epsilon_min, agent.epsilon)
        total_transitions = len(agent.memory.sum_tree.data)
        print(f"Episode {e+1}/{episodes} - Total Transitions: {total_transitions}")

        # Update best_net_worth
        if env_full.net_worth > best_net_worth + min_delta:
            best_net_worth = env_full.net_worth
            wait = 0
            # Gem model til output_dir
            model_path = os.path.join(output_dir, f'best_dqn_model_{timestamp}.pth')
            agent.save_model(model_path)
            logger.info("Best DQN model saved.")
            # Udskriv kun de ønskede metrics
            print(f"Episode {e+1}/{episodes} - Net_Worth: {env_full.net_worth:.2f}, Total_Reward: {total_reward:.2f}")
        else:
            wait += 1
            if wait >= patience:
                logger.info("Early stopping triggered.")
                print("Early stopping triggered.")
                break

        # Udskriv kun de ønskede metrics
        if env_full.net_worth <= best_net_worth + min_delta:
            print(f"Episode {e+1}/{episodes} - Net_Worth: {env_full.net_worth:.2f}, Total_Reward: {total_reward:.2f}")

        logger.info(f"Episode {e+1}/{episodes} afsluttet med Net Worth: {env_full.net_worth:.2f}, Total Reward: {total_reward:.2f}")

        # Evaluer på valideringssæt hvert 10. episode
        if (e + 1) % 10 == 0:
            average_val_profit = 0
            env_val = SimplifiedCryptoTradingEnv(df_val, transaction_cost=env_full.transaction_cost, scaler=scaler)
            for _ in range(10):
                state_val = env_val.reset()
                done_val = False
                while not done_val:
                    action_val = agent.act(state_val)
                    next_state_val, reward_val, done_val, info_val = env_val.step(action_val)
                    average_val_profit += reward_val
                    state_val = next_state_val
            average_val_profit /= 10
            logger.info(f"Validerings Profit efter episode {e+1}: {average_val_profit}")
            print(f"Validerings Profit efter episode {e+1}: {average_val_profit}")

            if average_val_profit > best_val_reward:
                best_val_reward = average_val_profit
                wait = 0
                # Gem model til output_dir
                model_path = os.path.join(output_dir, f'best_val_dqn_model_{timestamp}.pth')
                agent.save_model(model_path)
                logger.info("Best validation DQN model saved.")
                print("Best validation DQN model saved.")
            else:
                wait += 1
                if wait >= patience:
                    logger.info("Early stopping triggered by validation performance.")
                    print("Early stopping triggered by validation performance.")
                    break

    # Gem den endelige model efter træning
    final_model_path = os.path.join(output_dir, f'final_dqn_model_{timestamp}.pth')
    agent.save_model(final_model_path)
    logger.info(f"Endelig model gemt til {final_model_path}")
    print(f"Endelig model gemt til {final_model_path}")

    # Plot resultater og gem CSV-filer
    plot_training_results(episode_net_worths, episode_rewards, output_dir, csv_path=f'training_results_{timestamp}.csv')
    plot_trades(env_full.trade_history, df_train, output_dir, csv_path=f'trade_history_{timestamp}.csv')

    # Evaluer model på valideringssæt
    evaluate_model(df_val, agent, output_dir)

# ----------------------------
# Main Funktion til Lokalt Miljø
# ----------------------------
def main():
    """
    Hovedprogrammet koordinerer hele processen fra dataforberedelse til træning og evaluering med HER.
    """
    try:
        # Angiv stien til din CSV-fil lokalt
        data_path = r'/mnt/e/crypto/RF_model/agent/bootcamp/bitcoin_2024_5min_new.csv'  # Juster denne sti efter behov

        # Datafil kontrol
        if not os.path.exists(data_path):
            logger.error(f"Datafilen findes ikke: {data_path}")
            raise FileNotFoundError(f"Datafilen findes ikke: {data_path}. Tjek filstien og prøv igen.")

        df = pd.read_csv(data_path)
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])

        if df.isnull().any().any():
            logger.warning("Datasættet indeholder null-værdier. Fjerner disse rækker.")
            df = df.dropna()

        if len(df) == 0:
            raise ValueError("Datasættet er tomt efter forbehandling.")

        # Split data i træning og validering
        train_size = int(0.8 * len(df))
        df_train = df.iloc[:train_size].copy()
        df_val = df.iloc[train_size:].copy()

        # Tilføj tekniske indikatorer
        def add_indicators(data):
            # Moving Average Convergence Divergence (MACD)
            macd = trend.MACD(close=data['close'])
            data['macd'] = macd.macd()
            data['macd_diff'] = macd.macd_diff()
            data['macd_signal'] = macd.macd_signal()

            # Relative Strength Index (RSI)
            rsi = momentum.RSIIndicator(close=data['close'], window=14)
            data['rsi'] = rsi.rsi()

            # Average True Range (ATR)
            atr = AverageTrueRange(high=data['high'], low=data['low'], close=data['close'], window=14)
            data['atr'] = atr.average_true_range()

            # Ichimoku Cloud
            ichimoku = IchimokuIndicator(high=data['high'], low=data['low'], window1=9, window2=26, window3=52)
            data['ichimoku_a'] = ichimoku.ichimoku_a()
            data['ichimoku_b'] = ichimoku.ichimoku_b()
            data['ichimoku_base_line'] = ichimoku.ichimoku_base_line()
            data['ichimoku_conversion_line'] = ichimoku.ichimoku_conversion_line()

            # Eksponentielle glidende gennemsnit (EMA)
            data['ema_short'] = data['close'].ewm(span=12, adjust=False).mean()
            data['ema_long'] = data['close'].ewm(span=26, adjust=False).mean()
            data['ema_crossover'] = data['ema_short'] - data['ema_long']
            data['ema_crossover_lag1'] = data['ema_crossover'].shift(1)

           
            # Tilføj fler lag til features hvis nødvendigt
            # Eksempel: Lag 2 EMA
            #data['ema_2'] = data['close'].ewm(span=50, adjust=False).mean()

            data = data.dropna()
            return data

        df_train = add_indicators(df_train)
        df_val = add_indicators(df_val)

        # Gem unscaled 'close' pris til plotting og miljøet
        df_train['close_unscaled'] = df_train['close'].copy()
        df_val['close_unscaled'] = df_val['close'].copy()

        # Fjern 'timestamp' kolonnen, da den ikke er numerisk
        if 'timestamp' in df_train.columns:
            df_train = df_train.drop(columns=['timestamp'])
        if 'timestamp' in df_val.columns:
            df_val = df_val.drop(columns=['timestamp'])

        # Normaliser data inklusive 'close'
        scaler = MinMaxScaler()
        numeric_cols = df_train.select_dtypes(include=[np.number]).columns.tolist()
        if 'close_unscaled' in numeric_cols:
            numeric_cols.remove('close_unscaled')  # Fjern 'close_unscaled' fra skalering

        df_train[numeric_cols] = scaler.fit_transform(df_train[numeric_cols])
        df_val[numeric_cols] = scaler.transform(df_val[numeric_cols])

        logger.info(f"Træningsdata omfang: {df_train.shape[0]} rækker, {df_train.shape[1]} kolonner")
        logger.info(f"Træningsdata eksempel:\n{df_train.head()}")

        logger.info(f"Valideringsdata omfang: {df_val.shape[0]} rækker, {df_val.shape[1]} kolonner")
        logger.info(f"Valideringsdata eksempel:\n{df_val.head()}")

        # Kør Hovedtræning med HER
        logger.info("Starter hovedtræning med HER...")
        main_training(df_train, df_val, scaler, output_dir)
        logger.info("Hovedtræning færdig.")

    except FileNotFoundError as e:
        logger.error(e)
        sys.exit(1)
    except pd.errors.EmptyDataError:
        logger.error("CSV-filen er tom. Sørg for, at filen indeholder data.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Fejl i hovedprogrammet: {e}")
        sys.exit(1)
    finally:
        if n_gpus >= 1:
            try:
                pynvml.nvmlShutdown()
            except pynvml.NVMLError as e:
                logger.warning(f"Fejl ved shutdown af NVML: {e}")

        logger.info("Træning færdig.")

# ----------------------------
# Kør hovedprogrammet
# ----------------------------
if __name__ == "__main__":
    main()
